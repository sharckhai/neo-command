# VirtueCommand Backend + Pipeline Design (No FE / No Graph RAG)

## Architecture Overview
VirtueCommand will ship a backend-only runtime that exposes a FastAPI SSE chat endpoint, backed by a local-first data layer (DuckDB + LanceDB) with optional Databricks integrations. The pipeline remains the source of truth for data, producing `output/step4_entities.parquet` and `output/step4_embeddings.parquet` that the backend reads directly. The service will adopt the OpenAI Agents SDK for orchestration when an API key is present; otherwise it will fall back to a deterministic heuristic router so the API stays functional in local development. The Supervisor agent performs intent routing into three modes (EXPLORE / VERIFY / PLAN) and supplies a structured toolset: SQL queries, vector search, and verification checks. Each tool is wrapped with a tracing decorator that records a step-by-step trace suitable for the UI trace panel, even though the frontend is out of scope. MLflow integration is optional: if MLflow is configured, each request creates a run that logs tool calls, data sources, and summary metrics. If MLflow is not configured, traces are kept in-memory and returned in the SSE payload.

Databricks integrations are designed as optional adapters rather than mandatory dependencies. SQL execution uses DuckDB by default, with a Databricks SQL connector adapter if `DATABRICKS_HOST`, `DATABRICKS_TOKEN`, and `DATABRICKS_WAREHOUSE_ID` are present. Vector search uses LanceDB by default, with a Databricks Vector Search adapter when configured. Text-to-SQL is provided via a Genie client that defaults to a local OpenAI prompt if Genie is not configured. This allows the system to be fully demoable offline while still aligning with the sponsor stack when credentials are provided. Knowledge graph functionality is explicitly omitted; verification and gap analysis rely on the medical knowledge heuristics and derived rules instead.

## Agent Workflows
EXPLORE mode combines SQL and vector retrieval. The SQL tool answers structured questions (counts, region comparisons, facility types), while the vector tool searches free-text fields (procedure, equipment, capability, description) and then runs a self-reflective filter to remove referral-only or aspirational claims. Self-reflection is implemented as an LLM grading step that labels each candidate as "supported", "referral-only", or "aspirational", plus a confidence value. Only "supported" evidence is returned. VERIFY mode evaluates facility claims by checking for missing prerequisite equipment and breadth-depth mismatches. It then runs an Advocate/Skeptic debate (two LLM calls with opposing prompts) to generate a balanced analysis and a confidence score. PLAN mode synthesizes region-level indicators (facility density, rare procedures, confidence distribution, and equipment gaps) and uses a three-advocate debate to propose deployment targets. The synthesis step outputs a recommendation with explicit caveats and verification steps.

All three modes produce a response schema that includes citations (source URLs and data fields), map actions (region zoom and highlighted facilities), and a trace list describing each tool call. The backend also supports a single-step map-oriented query ("what's near here") by allowing a geographic filter against the entity table. Graceful failure handling is included at every stage: empty SQL results, missing embeddings, low confidence, and missing API keys are all surfaced as helpful responses rather than errors.

## Data, Tracing, and Testing
The pipeline stays largely intact, but it gains a Databricks upload adapter that can create Delta tables for entities and embeddings and (optionally) initiate a Vector Search index creation. The upload path is guarded so local developers can run without Databricks credentials, while production users can enable it via environment variables. The backend embeds user queries using `text-embedding-3-small` and uses the same embedding dimension as the pipeline. To keep the runtime fast, the API uses pre-computed parquet files for all metadata, and vector search only runs when the request includes a capability/service keyword or a free-text query.

Tracing is implemented with two layers: an in-memory trace list that is always returned in the SSE final payload, and an MLflow run logger that records structured metrics and inputs. Traces include step names, tool parameters, row counts, and truncated evidence snippets. This supports the trace panel UX without requiring the frontend. Testing focuses on deterministic tools and on the presence/shape of trace data rather than LLM output correctness. Unit tests cover the tool adapters (DuckDB, LanceDB), self-reflection filtering logic with a mocked LLM, and plan/verify summarizers. Integration tests validate the `/api/chat` endpoint behavior for each mode and ensure the response schema includes trace and citations.
