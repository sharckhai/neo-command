# VirtueCommand Pipeline Design

## Architecture
The pipeline is a local, uv-managed Python project that turns the raw Ghana facility CSV into clean, geocoded, enriched, and vectorized records. The structure follows a stepwise contract where each stage reads a parquet file from `output/` and writes the next, which makes the run idempotent and easy to resume. The project uses a `src/` layout with `pipeline/` for the orchestration and step logic, and `models/` for shared enums, region normalization, and Pydantic schemas. The CLI entry point is a Typer app that maps one command per step plus a full `run` command. External dependencies are minimal and explicit: pandas and pyarrow for data IO, pydantic for type alignment with existing models, openai for LLM enrichment, and rich/tenacity for progress and retries. The design intentionally keeps the data logic in pure Python to avoid hidden dependencies on Databricks or Spark during local development. Databricks upload is deferred to a separate ingestion step that consumes the parquet outputs. This matches the overall hackathon posture of fast iteration locally while keeping a clean integration boundary for the data platform. The pipeline is also designed to be testable with small subsets of the CSV and deterministic transformations for the non-LLM steps. Configuration is driven by environment variables for API keys and target behavior, with a `.env` file for local development.

## Components
Step 1 (`clean.py`) is the heavy lifting phase. It parses JSON-as-string fields into lists, normalizes facility types, derives source types from URL domains, and cleans missing or null markers. It also normalizes regions using a fixed mapping and city-based inference. Deduplication is done on `pk_unique_id` with a best-source policy for scalar fields and union semantics for list fields. Name selection uses a majority vote to reduce outliers, and quality flags capture conflicts, legacy region values, or name/address contamination. Step 2 (`geocode.py`) adds coordinates and confidence levels using a tiered approach: city lookups from a gazetteer, optional LLM city extraction from text, and a fallback to region centroids. Step 3 (`fingerprint.py`) builds a structured capability fingerprint with LLM support and computes a confidence score based on source quality, anomaly flags, and evidence density. Step 4 (`embed.py`) turns each facility into multiple field-specific chunks and creates embeddings with a consistent context prefix so search is aligned to facility intent. Step 5 (`upload.py`) defaults to LanceDB for local usage, with a clear boundary for Databricks ingestion. Each step emits parquet files to support downstream consumers and traceability.

## Data Flow, Error Handling, and Testing
Every step is built around an explicit input and output file to avoid accidental hidden state. The main flow is `raw csv -> step1_clean -> step2_geocoded -> step3_fingerprinted -> step4_entities + step4_embeddings -> upload`. Step 1 handles parse errors by dropping invalid list entries and emitting quality flags rather than failing the run. Step 2 handles missing coordinates by assigning a `geocode_confidence` of `unknown` and leaving lat/lng empty, which allows downstream consumers to filter without breaking. LLM calls in Steps 2 and 3 use retries with exponential backoff and fall back to minimal fingerprints if the API key is missing or a request fails. Step 4 is deterministic given the fingerprint outputs and includes batching for embeddings to control cost. For verification, each step can be spot-checked by inspecting row counts, unique regions, and embedding dimensions in a small notebook or a simple validation script. The CLI includes step-level commands so developers can re-run only the segment they are working on. A minimal set of automated checks can be added later to assert schema consistency and basic row count expectations, but the pipeline is already structured to make manual validation fast and predictable.
